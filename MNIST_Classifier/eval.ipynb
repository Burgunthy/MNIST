{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST_Classifier_eval.ipynb","provenance":[],"mount_file_id":"1aYXc6bEKI76M-onl2YwsINq9y2lHdM0K","authorship_tag":"ABX9TyNe8MCDQq0SVzouvNIQFrPC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8hgyeaypQINR","executionInfo":{"status":"ok","timestamp":1601374456077,"user_tz":-540,"elapsed":1407,"user":{"displayName":"정태현/학생/정보통신공학","photoUrl":"","userId":"08740959862878839545"}}},"source":["import os\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from torchvision import transforms, datasets"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLLfOfoZsHE7","executionInfo":{"status":"ok","timestamp":1601374456080,"user_tz":-540,"elapsed":1401,"user":{"displayName":"정태현/학생/정보통신공학","photoUrl":"","userId":"08740959862878839545"}}},"source":["# parameter 설정\n","\n","lr = 1e-3\n","batch_size = 64\n","num_epoch = 10\n","\n","ckpt_dir = './drive/My Drive/Colab Notebooks/MNIST_Test/checkpoint'     # 체크 포인트. 주기마다 모델 저장\n","log_dir = './drive/My Drive/Colab Notebooks/MNIST_Test/log'             # 텐서 보드 확인을 위한 로그 저장\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # CUDA 디바이스 설정\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"8PPNIfEbuBPE","executionInfo":{"status":"ok","timestamp":1601374456082,"user_tz":-540,"elapsed":1398,"user":{"displayName":"정태현/학생/정보통신공학","photoUrl":"","userId":"08740959862878839545"}}},"source":["# Networt 구축\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1, padding=0, bias=True)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2)\n","        self.relu1 = nn.ReLU()\n","\n","        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1, padding=0, bias=True)\n","        self.drop2 = nn.Dropout2d(p=0.5)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2)\n","        self.relu2 = nn.ReLU()\n","\n","        self.fc1 = nn.Linear(in_features=320, out_features=50, bias=True)\n","        self.relu1_fc1 = nn.ReLU()\n","        self.drop1_fc1 = nn.Dropout2d(p=0.5)\n","\n","        self.fc2 = nn.Linear(in_features=50, out_features=10, bias=True)      # 출력부 10으로 데이터 확인\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.pool1(x)\n","        x = self.relu1(x)\n","\n","        x = self.conv2(x)\n","        x = self.drop2(x)\n","        x = self.pool2(x)\n","        x = self.relu2(x)\n","\n","        x = x.view(-1, 320)\n","\n","        x = self.fc1(x)\n","        x = self.relu1_fc1(x)\n","        x = self.drop1_fc1(x)\n","\n","        x = self.fc2(x)\n","\n","        return x"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJewdKNZVfkG","executionInfo":{"status":"ok","timestamp":1601374456492,"user_tz":-540,"elapsed":1803,"user":{"displayName":"정태현/학생/정보통신공학","photoUrl":"","userId":"08740959862878839545"}}},"source":["# 네트워크 저장 함수\n","\n","def save(ckpt_dir, net, optim, epoch):        # check point 저장\n","    if not os.path.exists(ckpt_dir):\n","        os.makedirs(ckpt_dir)                 # 없다면 체크 포인트 생성\n","\n","    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},    # net, optim -> pth로 저장\n","               './%s/model_epoch%d.pth' % (ckpt_dir, epoch))              # net : , optim : \n","\n","def load(ckpt_dir, net, optim):               # check point 로드\n","    ckpt_lst = os.listdir(ckpt_dir)\n","    ckpt_lst.sort()                           # check point list 한 후 마지막 사용을 위해 sort -> 1, 10도 sort가 되는지 확인\n","\n","    dict_model = torch.load('./%s/%s' % (ckpt_dir, ckpt_lst[-1]))         # 마지막 sort를 사용. dict_model에 넣어준다\n","\n","    net.load_state_dict(dict_model['net'])                                # net, optim 모델에서 로드\n","    optim.load_state_dict(dict_model['optim'])\n","\n","    return net, optim"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"ngzA8FC6Wt4m","executionInfo":{"status":"ok","timestamp":1601374456494,"user_tz":-540,"elapsed":1799,"user":{"displayName":"정태현/학생/정보통신공학","photoUrl":"","userId":"08740959862878839545"}}},"source":["# MNIST 데이터 불러오기\n","\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])      # 텐서 바꾸기 + Normalization = transform\n","\n","dataset = datasets.MNIST(download=True, root='./drive/My Drive/Colab Notebooks/MNIST_Test', train=False, transform=transform)  # False 변경\n","loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)                             # False 변경\n","\n","num_data = len(loader.dataset)                      # dataset 개수 확인\n","num_batch = np.ceil(num_data / batch_size)          # dataset 개수에서 batch_size를 나눠 training batch 개수 구함"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qb9lH7oSXqqe","executionInfo":{"status":"ok","timestamp":1601374456496,"user_tz":-540,"elapsed":1796,"user":{"displayName":"정태현/학생/정보통신공학","photoUrl":"","userId":"08740959862878839545"}}},"source":["# 네트워크 설정, 손실함수 구현\n","\n","net = Net().to(device)                              # 앞서 설정한 Net 구조 사용 변수\n","params = net.parameters()                           # Net의 변수 저장\n","\n","fn_loss = nn.CrossEntropyLoss().to(device)          # loss 함수. CrossEntropyLoss 사용\n","fn_pred = lambda output: torch.softmax(output, dim=1)         # softmax 사용 predict 모델 생성 변수\n","fn_acc = lambda pred, label: ((pred.max(dim=1)[1] == label).type(torch.float)).mean()           # 예측 모델과 실제 모델을 합쳐 정확성 체크\n","\n","optim = torch.optim.Adam(params, lr=lr)             # Adam 사용. Adam 논문 정독 필요\n","\n","writer = SummaryWriter(log_dir=log_dir)             # log 저장\n","\n","# 네트워크 로드 부분 작성\n","\n","net, optim = load(ckpt_dir=ckpt_dir, net=net, optim=optim)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jV_h_aRZ-V7","executionInfo":{"status":"ok","timestamp":1601374458902,"user_tz":-540,"elapsed":4196,"user":{"displayName":"정태현/학생/정보통신공학","photoUrl":"","userId":"08740959862878839545"}},"outputId":"e9982084-6543-44ba-f25e-c3c5319f5cb7","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 트레이닝\n","\n","# for epoch in range(1, num_epoch + 1):               # epoch 제거\n","with torch.no_grad():\n","    # net.train()\n","    net.eval()                                      # eval로 변경\n","\n","    loss_arr = []                                   # Loss 함수 저장\n","    acc_arr = []                                    # 정확도 함수 저장\n","\n","    for  batch, (input, label) in enumerate(loader, 1):     # batch 마다 진행\n","        input = input.to(device)\n","        label = label.to(device)\n","\n","        output = net(input)                         # Net에 input\n","        pred = fn_pred(output)                      # 구한 output softmax\n","\n","        # optim.zero_grad()                           # Eval이므로 없앰\n","\n","        loss = fn_loss(output, label)\n","        acc = fn_acc(pred, label)\n","\n","        # loss.backward()                             # Eval이므로 없앰\n","\n","        # optim.step()                                # Eval이므로 없앰\n","\n","        loss_arr += [loss.item()]                   # Tensorboard 확인을 위한 loss 저장\n","        acc_arr += [acc.item()]                     # Tensorboard 확인을 위한 acc 저장\n","\n","        print('TRAIN: BATCH %04d/%04d | LOSS: %.4f | ACC %.4f' %\n","              (batch, num_batch, np.mean(loss_arr), np.mean(acc_arr)))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["TRAIN: BATCH 0001/0157 | LOSS: 0.0023 | ACC 1.0000\n","TRAIN: BATCH 0002/0157 | LOSS: 0.0053 | ACC 1.0000\n","TRAIN: BATCH 0003/0157 | LOSS: 0.0085 | ACC 1.0000\n","TRAIN: BATCH 0004/0157 | LOSS: 0.0218 | ACC 0.9961\n","TRAIN: BATCH 0005/0157 | LOSS: 0.0219 | ACC 0.9969\n","TRAIN: BATCH 0006/0157 | LOSS: 0.0281 | ACC 0.9922\n","TRAIN: BATCH 0007/0157 | LOSS: 0.0347 | ACC 0.9888\n","TRAIN: BATCH 0008/0157 | LOSS: 0.0386 | ACC 0.9883\n","TRAIN: BATCH 0009/0157 | LOSS: 0.0396 | ACC 0.9878\n","TRAIN: BATCH 0010/0157 | LOSS: 0.0456 | ACC 0.9875\n","TRAIN: BATCH 0011/0157 | LOSS: 0.0479 | ACC 0.9858\n","TRAIN: BATCH 0012/0157 | LOSS: 0.0497 | ACC 0.9844\n","TRAIN: BATCH 0013/0157 | LOSS: 0.0464 | ACC 0.9856\n","TRAIN: BATCH 0014/0157 | LOSS: 0.0441 | ACC 0.9866\n","TRAIN: BATCH 0015/0157 | LOSS: 0.0478 | ACC 0.9865\n","TRAIN: BATCH 0016/0157 | LOSS: 0.0499 | ACC 0.9854\n","TRAIN: BATCH 0017/0157 | LOSS: 0.0507 | ACC 0.9844\n","TRAIN: BATCH 0018/0157 | LOSS: 0.0506 | ACC 0.9844\n","TRAIN: BATCH 0019/0157 | LOSS: 0.0492 | ACC 0.9852\n","TRAIN: BATCH 0020/0157 | LOSS: 0.0539 | ACC 0.9820\n","TRAIN: BATCH 0021/0157 | LOSS: 0.0562 | ACC 0.9814\n","TRAIN: BATCH 0022/0157 | LOSS: 0.0555 | ACC 0.9815\n","TRAIN: BATCH 0023/0157 | LOSS: 0.0544 | ACC 0.9817\n","TRAIN: BATCH 0024/0157 | LOSS: 0.0585 | ACC 0.9811\n","TRAIN: BATCH 0025/0157 | LOSS: 0.0593 | ACC 0.9806\n","TRAIN: BATCH 0026/0157 | LOSS: 0.0612 | ACC 0.9808\n","TRAIN: BATCH 0027/0157 | LOSS: 0.0656 | ACC 0.9797\n","TRAIN: BATCH 0028/0157 | LOSS: 0.0654 | ACC 0.9799\n","TRAIN: BATCH 0029/0157 | LOSS: 0.0635 | ACC 0.9806\n","TRAIN: BATCH 0030/0157 | LOSS: 0.0659 | ACC 0.9802\n","TRAIN: BATCH 0031/0157 | LOSS: 0.0643 | ACC 0.9808\n","TRAIN: BATCH 0032/0157 | LOSS: 0.0676 | ACC 0.9795\n","TRAIN: BATCH 0033/0157 | LOSS: 0.0668 | ACC 0.9792\n","TRAIN: BATCH 0034/0157 | LOSS: 0.0675 | ACC 0.9789\n","TRAIN: BATCH 0035/0157 | LOSS: 0.0683 | ACC 0.9786\n","TRAIN: BATCH 0036/0157 | LOSS: 0.0676 | ACC 0.9787\n","TRAIN: BATCH 0037/0157 | LOSS: 0.0659 | ACC 0.9793\n","TRAIN: BATCH 0038/0157 | LOSS: 0.0676 | ACC 0.9790\n","TRAIN: BATCH 0039/0157 | LOSS: 0.0668 | ACC 0.9792\n","TRAIN: BATCH 0040/0157 | LOSS: 0.0652 | ACC 0.9797\n","TRAIN: BATCH 0041/0157 | LOSS: 0.0667 | ACC 0.9794\n","TRAIN: BATCH 0042/0157 | LOSS: 0.0679 | ACC 0.9792\n","TRAIN: BATCH 0043/0157 | LOSS: 0.0665 | ACC 0.9797\n","TRAIN: BATCH 0044/0157 | LOSS: 0.0656 | ACC 0.9801\n","TRAIN: BATCH 0045/0157 | LOSS: 0.0643 | ACC 0.9806\n","TRAIN: BATCH 0046/0157 | LOSS: 0.0653 | ACC 0.9800\n","TRAIN: BATCH 0047/0157 | LOSS: 0.0647 | ACC 0.9801\n","TRAIN: BATCH 0048/0157 | LOSS: 0.0645 | ACC 0.9801\n","TRAIN: BATCH 0049/0157 | LOSS: 0.0637 | ACC 0.9805\n","TRAIN: BATCH 0050/0157 | LOSS: 0.0626 | ACC 0.9809\n","TRAIN: BATCH 0051/0157 | LOSS: 0.0619 | ACC 0.9810\n","TRAIN: BATCH 0052/0157 | LOSS: 0.0616 | ACC 0.9811\n","TRAIN: BATCH 0053/0157 | LOSS: 0.0609 | ACC 0.9811\n","TRAIN: BATCH 0054/0157 | LOSS: 0.0604 | ACC 0.9812\n","TRAIN: BATCH 0055/0157 | LOSS: 0.0608 | ACC 0.9810\n","TRAIN: BATCH 0056/0157 | LOSS: 0.0640 | ACC 0.9807\n","TRAIN: BATCH 0057/0157 | LOSS: 0.0636 | ACC 0.9808\n","TRAIN: BATCH 0058/0157 | LOSS: 0.0628 | ACC 0.9811\n","TRAIN: BATCH 0059/0157 | LOSS: 0.0647 | ACC 0.9809\n","TRAIN: BATCH 0060/0157 | LOSS: 0.0660 | ACC 0.9802\n","TRAIN: BATCH 0061/0157 | LOSS: 0.0661 | ACC 0.9798\n","TRAIN: BATCH 0062/0157 | LOSS: 0.0668 | ACC 0.9796\n","TRAIN: BATCH 0063/0157 | LOSS: 0.0666 | ACC 0.9794\n","TRAIN: BATCH 0064/0157 | LOSS: 0.0663 | ACC 0.9795\n","TRAIN: BATCH 0065/0157 | LOSS: 0.0659 | ACC 0.9793\n","TRAIN: BATCH 0066/0157 | LOSS: 0.0660 | ACC 0.9792\n","TRAIN: BATCH 0067/0157 | LOSS: 0.0664 | ACC 0.9788\n","TRAIN: BATCH 0068/0157 | LOSS: 0.0657 | ACC 0.9791\n","TRAIN: BATCH 0069/0157 | LOSS: 0.0651 | ACC 0.9792\n","TRAIN: BATCH 0070/0157 | LOSS: 0.0646 | ACC 0.9792\n","TRAIN: BATCH 0071/0157 | LOSS: 0.0651 | ACC 0.9789\n","TRAIN: BATCH 0072/0157 | LOSS: 0.0659 | ACC 0.9785\n","TRAIN: BATCH 0073/0157 | LOSS: 0.0660 | ACC 0.9786\n","TRAIN: BATCH 0074/0157 | LOSS: 0.0654 | ACC 0.9789\n","TRAIN: BATCH 0075/0157 | LOSS: 0.0650 | ACC 0.9790\n","TRAIN: BATCH 0076/0157 | LOSS: 0.0663 | ACC 0.9786\n","TRAIN: BATCH 0077/0157 | LOSS: 0.0664 | ACC 0.9783\n","TRAIN: BATCH 0078/0157 | LOSS: 0.0666 | ACC 0.9784\n","TRAIN: BATCH 0079/0157 | LOSS: 0.0658 | ACC 0.9786\n","TRAIN: BATCH 0080/0157 | LOSS: 0.0651 | ACC 0.9787\n","TRAIN: BATCH 0081/0157 | LOSS: 0.0644 | ACC 0.9790\n","TRAIN: BATCH 0082/0157 | LOSS: 0.0637 | ACC 0.9792\n","TRAIN: BATCH 0083/0157 | LOSS: 0.0629 | ACC 0.9795\n","TRAIN: BATCH 0084/0157 | LOSS: 0.0622 | ACC 0.9797\n","TRAIN: BATCH 0085/0157 | LOSS: 0.0615 | ACC 0.9800\n","TRAIN: BATCH 0086/0157 | LOSS: 0.0608 | ACC 0.9802\n","TRAIN: BATCH 0087/0157 | LOSS: 0.0601 | ACC 0.9804\n","TRAIN: BATCH 0088/0157 | LOSS: 0.0595 | ACC 0.9806\n","TRAIN: BATCH 0089/0157 | LOSS: 0.0589 | ACC 0.9809\n","TRAIN: BATCH 0090/0157 | LOSS: 0.0585 | ACC 0.9811\n","TRAIN: BATCH 0091/0157 | LOSS: 0.0579 | ACC 0.9813\n","TRAIN: BATCH 0092/0157 | LOSS: 0.0573 | ACC 0.9815\n","TRAIN: BATCH 0093/0157 | LOSS: 0.0578 | ACC 0.9815\n","TRAIN: BATCH 0094/0157 | LOSS: 0.0580 | ACC 0.9815\n","TRAIN: BATCH 0095/0157 | LOSS: 0.0575 | ACC 0.9817\n","TRAIN: BATCH 0096/0157 | LOSS: 0.0571 | ACC 0.9818\n","TRAIN: BATCH 0097/0157 | LOSS: 0.0571 | ACC 0.9818\n","TRAIN: BATCH 0098/0157 | LOSS: 0.0565 | ACC 0.9820\n","TRAIN: BATCH 0099/0157 | LOSS: 0.0560 | ACC 0.9822\n","TRAIN: BATCH 0100/0157 | LOSS: 0.0554 | ACC 0.9823\n","TRAIN: BATCH 0101/0157 | LOSS: 0.0549 | ACC 0.9825\n","TRAIN: BATCH 0102/0157 | LOSS: 0.0546 | ACC 0.9825\n","TRAIN: BATCH 0103/0157 | LOSS: 0.0554 | ACC 0.9819\n","TRAIN: BATCH 0104/0157 | LOSS: 0.0575 | ACC 0.9817\n","TRAIN: BATCH 0105/0157 | LOSS: 0.0569 | ACC 0.9818\n","TRAIN: BATCH 0106/0157 | LOSS: 0.0566 | ACC 0.9819\n","TRAIN: BATCH 0107/0157 | LOSS: 0.0561 | ACC 0.9820\n","TRAIN: BATCH 0108/0157 | LOSS: 0.0557 | ACC 0.9822\n","TRAIN: BATCH 0109/0157 | LOSS: 0.0552 | ACC 0.9824\n","TRAIN: BATCH 0110/0157 | LOSS: 0.0547 | ACC 0.9825\n","TRAIN: BATCH 0111/0157 | LOSS: 0.0543 | ACC 0.9825\n","TRAIN: BATCH 0112/0157 | LOSS: 0.0539 | ACC 0.9826\n","TRAIN: BATCH 0113/0157 | LOSS: 0.0536 | ACC 0.9827\n","TRAIN: BATCH 0114/0157 | LOSS: 0.0532 | ACC 0.9829\n","TRAIN: BATCH 0115/0157 | LOSS: 0.0527 | ACC 0.9830\n","TRAIN: BATCH 0116/0157 | LOSS: 0.0523 | ACC 0.9832\n","TRAIN: BATCH 0117/0157 | LOSS: 0.0523 | ACC 0.9832\n","TRAIN: BATCH 0118/0157 | LOSS: 0.0519 | ACC 0.9833\n","TRAIN: BATCH 0119/0157 | LOSS: 0.0515 | ACC 0.9835\n","TRAIN: BATCH 0120/0157 | LOSS: 0.0510 | ACC 0.9836\n","TRAIN: BATCH 0121/0157 | LOSS: 0.0506 | ACC 0.9837\n","TRAIN: BATCH 0122/0157 | LOSS: 0.0503 | ACC 0.9839\n","TRAIN: BATCH 0123/0157 | LOSS: 0.0501 | ACC 0.9840\n","TRAIN: BATCH 0124/0157 | LOSS: 0.0499 | ACC 0.9840\n","TRAIN: BATCH 0125/0157 | LOSS: 0.0495 | ACC 0.9841\n","TRAIN: BATCH 0126/0157 | LOSS: 0.0491 | ACC 0.9843\n","TRAIN: BATCH 0127/0157 | LOSS: 0.0491 | ACC 0.9843\n","TRAIN: BATCH 0128/0157 | LOSS: 0.0488 | ACC 0.9844\n","TRAIN: BATCH 0129/0157 | LOSS: 0.0485 | ACC 0.9845\n","TRAIN: BATCH 0130/0157 | LOSS: 0.0482 | ACC 0.9846\n","TRAIN: BATCH 0131/0157 | LOSS: 0.0480 | ACC 0.9846\n","TRAIN: BATCH 0132/0157 | LOSS: 0.0477 | ACC 0.9847\n","TRAIN: BATCH 0133/0157 | LOSS: 0.0474 | ACC 0.9848\n","TRAIN: BATCH 0134/0157 | LOSS: 0.0471 | ACC 0.9850\n","TRAIN: BATCH 0135/0157 | LOSS: 0.0467 | ACC 0.9851\n","TRAIN: BATCH 0136/0157 | LOSS: 0.0464 | ACC 0.9852\n","TRAIN: BATCH 0137/0157 | LOSS: 0.0460 | ACC 0.9853\n","TRAIN: BATCH 0138/0157 | LOSS: 0.0457 | ACC 0.9854\n","TRAIN: BATCH 0139/0157 | LOSS: 0.0454 | ACC 0.9855\n","TRAIN: BATCH 0140/0157 | LOSS: 0.0450 | ACC 0.9856\n","TRAIN: BATCH 0141/0157 | LOSS: 0.0458 | ACC 0.9854\n","TRAIN: BATCH 0142/0157 | LOSS: 0.0458 | ACC 0.9854\n","TRAIN: BATCH 0143/0157 | LOSS: 0.0455 | ACC 0.9855\n","TRAIN: BATCH 0144/0157 | LOSS: 0.0452 | ACC 0.9856\n","TRAIN: BATCH 0145/0157 | LOSS: 0.0449 | ACC 0.9857\n","TRAIN: BATCH 0146/0157 | LOSS: 0.0447 | ACC 0.9857\n","TRAIN: BATCH 0147/0157 | LOSS: 0.0444 | ACC 0.9858\n","TRAIN: BATCH 0148/0157 | LOSS: 0.0441 | ACC 0.9859\n","TRAIN: BATCH 0149/0157 | LOSS: 0.0438 | ACC 0.9859\n","TRAIN: BATCH 0150/0157 | LOSS: 0.0436 | ACC 0.9860\n","TRAIN: BATCH 0151/0157 | LOSS: 0.0440 | ACC 0.9859\n","TRAIN: BATCH 0152/0157 | LOSS: 0.0442 | ACC 0.9858\n","TRAIN: BATCH 0153/0157 | LOSS: 0.0445 | ACC 0.9857\n","TRAIN: BATCH 0154/0157 | LOSS: 0.0443 | ACC 0.9857\n","TRAIN: BATCH 0155/0157 | LOSS: 0.0448 | ACC 0.9855\n","TRAIN: BATCH 0156/0157 | LOSS: 0.0446 | ACC 0.9856\n","TRAIN: BATCH 0157/0157 | LOSS: 0.0443 | ACC 0.9857\n"],"name":"stdout"}]}]}